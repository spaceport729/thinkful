{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validating a Naive Bayes Classifier used to Perform Sentiment Analysis\n",
    "\n",
    "Dataset: UCI sentiment labeled sentences\n",
    " https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences\n",
    " \n",
    "Positive/Negative word list: University of Pittsburgh Subjectivity Lexicon http://mpqa.cs.pitt.edu/ \n",
    "Instructions: \n",
    "1. Pick one of the company data files and build your own classifier. \n",
    "2. When you're satisfied with its performance (at this point just using the accuracy measure shown in the example), test it on one of the other datasets to see how well these kinds of classifiers translate from one context to another.\n",
    "3. Include your model and a brief writeup of your feature engineering and selection process to submit and review with your mentor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import config\n",
    "import string\n",
    "\n",
    "# data is binary so I'll use the Bernoulli classifier.\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abounds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abundance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abundant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pos_words\n",
       "0         a+\n",
       "1     abound\n",
       "2    abounds\n",
       "3  abundance\n",
       "4   abundant"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load text file of negative and positive words from http://mpqa.cs.pitt.edu/\n",
    "\n",
    "df_positive_words = pd.read_csv('positive-words.txt', header = None)\n",
    "df_positive_words.columns=['pos_words']\n",
    "df_negative_words = pd.read_csv('negative-words2.txt', header = None, encoding = \"ISO-8859-1\")\n",
    "df_negative_words.columns=['neg_words']\n",
    "\n",
    "df_positive_words.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "      <td>Wow Loved this place</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "      <td>Crust is not good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "      <td>Not tasty and the texture was just nasty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Now I am getting angry and I want my damn pho.</td>\n",
       "      <td>0</td>\n",
       "      <td>Now I am getting angry and I want my damn pho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Honeslty it didn't taste THAT fresh.)</td>\n",
       "      <td>0</td>\n",
       "      <td>Honeslty it didnt taste THAT fresh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The potatoes were like rubber and you could te...</td>\n",
       "      <td>0</td>\n",
       "      <td>The potatoes were like rubber and you could te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The fries were great too.</td>\n",
       "      <td>1</td>\n",
       "      <td>The fries were great too</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A great touch.</td>\n",
       "      <td>1</td>\n",
       "      <td>A great touch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  sentiment  \\\n",
       "0                           Wow... Loved this place.          1   \n",
       "1                                 Crust is not good.          0   \n",
       "2          Not tasty and the texture was just nasty.          0   \n",
       "3  Stopped by during the late May bank holiday of...          1   \n",
       "4  The selection on the menu was great and so wer...          1   \n",
       "5     Now I am getting angry and I want my damn pho.          0   \n",
       "6              Honeslty it didn't taste THAT fresh.)          0   \n",
       "7  The potatoes were like rubber and you could te...          0   \n",
       "8                          The fries were great too.          1   \n",
       "9                                     A great touch.          1   \n",
       "\n",
       "                                     message_cleaned  \n",
       "0                               Wow Loved this place  \n",
       "1                                  Crust is not good  \n",
       "2           Not tasty and the texture was just nasty  \n",
       "3  Stopped by during the late May bank holiday of...  \n",
       "4  The selection on the menu was great and so wer...  \n",
       "5      Now I am getting angry and I want my damn pho  \n",
       "6                 Honeslty it didnt taste THAT fresh  \n",
       "7  The potatoes were like rubber and you could te...  \n",
       "8                           The fries were great too  \n",
       "9                                      A great touch  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load sentiment data and label columns\n",
    "sentiment_raw = pd.read_csv(filepath_or_buffer='yelp_labelled.txt', delimiter='\\t', header=None)\n",
    "# name new columns\n",
    "sentiment_raw.columns=['message', 'sentiment']\n",
    "\n",
    "sentiment_raw['message_cleaned'] = sentiment_raw['message'].apply(lambda x:''.join([i for i in x \n",
    "                                                  if i not in string.punctuation]))\n",
    "sentiment_raw.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for class imbalance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of positive sentiments in initial training data:\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# check to see if there is a dominant class\n",
    "print(\"Proportion of positive sentiments in initial training data:\")\n",
    "print(sentiment_raw['sentiment'].sum()/1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are equal numbers of positive and negative sentiments in the original (entire dataset). The training data should not have been impacted by class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a+</th>\n",
       "      <th>abound</th>\n",
       "      <th>abounds</th>\n",
       "      <th>abundance</th>\n",
       "      <th>abundant</th>\n",
       "      <th>accessable</th>\n",
       "      <th>accessible</th>\n",
       "      <th>acclaim</th>\n",
       "      <th>acclaimed</th>\n",
       "      <th>acclamation</th>\n",
       "      <th>...</th>\n",
       "      <th>wrongly</th>\n",
       "      <th>wrought</th>\n",
       "      <th>yawn</th>\n",
       "      <th>zap</th>\n",
       "      <th>zapped</th>\n",
       "      <th>zaps</th>\n",
       "      <th>zealot</th>\n",
       "      <th>zealous</th>\n",
       "      <th>zealously</th>\n",
       "      <th>zombie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 5037 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   a+  abound  abounds  abundance  abundant  accessable  accessible  acclaim  \\\n",
       "0   0       0        0          0         0           0           0        0   \n",
       "1   0       0        0          0         0           0           0        0   \n",
       "2   0       0        0          0         0           0           0        0   \n",
       "3   0       0        0          0         0           0           0        0   \n",
       "4   0       0        0          0         0           0           0        0   \n",
       "\n",
       "   acclaimed  acclamation   ...    wrongly  wrought  yawn  zap  zapped  zaps  \\\n",
       "0          0            0   ...          0        0     0    0       0     0   \n",
       "1          0            0   ...          0        0     0    0       0     0   \n",
       "2          0            0   ...          0        0     0    0       0     0   \n",
       "3          0            0   ...          0        0     0    0       0     0   \n",
       "4          0            0   ...          0        0     0    0       0     0   \n",
       "\n",
       "   zealot  zealous  zealously  zombie  \n",
       "0       0        0          0       0  \n",
       "1       0        0          0       0  \n",
       "2       0        0          0       0  \n",
       "3       0        0          0       0  \n",
       "4       0        0          0       0  \n",
       "\n",
       "[5 rows x 5037 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#create an initial dataframe to get frequency of words from positive text file\n",
    "# positive_features = pd.DataFrame()\n",
    "# negative_features = pd.DataFrame()\n",
    "\n",
    "\n",
    "#create a series for negative words and for positive words using the text files \n",
    "\n",
    "keywords_positive = df_positive_words['pos_words']\n",
    "keywords_negative = df_negative_words['neg_words']\n",
    "\n",
    "\n",
    "#create a binary feature for the presence of positive words\n",
    "data = pd.DataFrame()\n",
    "for key in keywords_positive:\n",
    "    # spaces around the key to get the word,not just pattern matching.\n",
    "    data[str(key)] = sentiment_raw.message_cleaned.str.contains(' ' + str(key) + ' ', case=False).astype(int)\n",
    "\n",
    "for key in keywords_negative:\n",
    "    # spaces around the key to get the word,not just pattern matching.\n",
    "    data[str(key)] = sentiment_raw.message_cleaned.str.contains(' ' + str(key) + ' ', case=False).astype(int)\n",
    "\n",
    "data.head()\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a+               194\n",
      "good              62\n",
      "like              43\n",
      "great             43\n",
      "friendly          21\n",
      "nice              19\n",
      "love              16\n",
      "best              16\n",
      "pretty            15\n",
      "better            12\n",
      "recommend         11\n",
      "worth             10\n",
      "enough            10\n",
      "loved              9\n",
      "amazing            9\n",
      "worst              9\n",
      "fresh              9\n",
      "well               9\n",
      "happy              8\n",
      "hot                8\n",
      "excellent          8\n",
      "perfect            8\n",
      "right              7\n",
      "clean              7\n",
      "awesome            7\n",
      "super              7\n",
      "warm               6\n",
      "authentic          6\n",
      "enjoy              6\n",
      "slow               6\n",
      "                ... \n",
      "occlude            0\n",
      "occluded           0\n",
      "occludes           0\n",
      "occluding          0\n",
      "odd                0\n",
      "odder              0\n",
      "oddest             0\n",
      "oddities           0\n",
      "oddity             0\n",
      "oddly              0\n",
      "obstructed         0\n",
      "obstinately        0\n",
      "oblivious          0\n",
      "obstinate          0\n",
      "obnoxious          0\n",
      "obnoxiously        0\n",
      "obscene            0\n",
      "obscenely          0\n",
      "obscenity          0\n",
      "obscure            0\n",
      "obscured           0\n",
      "obscures           0\n",
      "obscurity          0\n",
      "obsess             0\n",
      "obsessive          0\n",
      "obsessively        0\n",
      "obsessiveness      0\n",
      "obsolete           0\n",
      "obstacle           0\n",
      "incivility         0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stace\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:3: FutureWarning: sort is deprecated, use sort_values(inplace=True) for INPLACE sorting\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# return most common sentiments used in training data\n",
    "sums = data.sum(axis=0)\n",
    "sums.sort(ascending=False)\n",
    "print(sums)\n",
    "\n",
    "data.drop(['a+'],axis=1,inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 1000 points : 251\n"
     ]
    }
   ],
   "source": [
    "# build training data as new dataframe for model and assign target (outcome variable)\n",
    "target = sentiment_raw['sentiment']\n",
    "\n",
    "# Instantiate our model and store it in a new variable.\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "# Fit our model to the data.\n",
    "bnb.fit(data, target)\n",
    "\n",
    "# Classify, storing the result in a new variable.\n",
    "y_pred = bnb.predict(data)\n",
    "\n",
    "# Display our results.\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(\n",
    "    data.shape[0],\n",
    "    (target != y_pred).sum()\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Use Cross-Validation to test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Fold  1 : Number of mislabeled points out of a total 100 points : 36\n",
      "Test Fold  2 : Number of mislabeled points out of a total 100 points : 35\n",
      "Test Fold  3 : Number of mislabeled points out of a total 100 points : 31\n",
      "Test Fold  4 : Number of mislabeled points out of a total 100 points : 34\n",
      "Test Fold  5 : Number of mislabeled points out of a total 100 points : 35\n",
      "Test Fold  6 : Number of mislabeled points out of a total 100 points : 27\n",
      "Test Fold  7 : Number of mislabeled points out of a total 100 points : 29\n",
      "Test Fold  8 : Number of mislabeled points out of a total 100 points : 42\n",
      "Test Fold  9 : Number of mislabeled points out of a total 100 points : 24\n",
      "Test Fold  10 : Number of mislabeled points out of a total 100 points : 12\n",
      "30.5\n"
     ]
    }
   ],
   "source": [
    "# create variants of feature sets to test with cross validation\n",
    "#try dropping uncommon feature values\n",
    "data.drop([col for col, val in data.sum().iteritems() if val < 1], axis=1, inplace=True)\n",
    "\n",
    "#try dropping a+ feature\n",
    "\n",
    "# try adding allcaps feature\n",
    "#data['allcaps'] = sentiment_raw.message_cleaned.str.isupper()\n",
    "\n",
    "# try adding exclamation points as feature\n",
    "#\n",
    "data_2 = data\n",
    "#identify number of folds and split the dataset\n",
    "n = 10\n",
    "data_2_split = np.array_split(data, n)\n",
    "target_split = np.array_split(target, n)\n",
    "\n",
    "#for each fold assign it as the test dataset and fit your model on the remaining data\n",
    "sum = 0\n",
    "for x in range(n):\n",
    "    train_data = []\n",
    "    train_target = []\n",
    "   \n",
    "    for y in range(n):\n",
    "        if y == x :\n",
    "            test_data = data_2_split[x]\n",
    "            test_target = target_split[y]\n",
    "        else:\n",
    "            train_data.append(data_2_split[y])\n",
    "            train_target.append(target_split[y])\n",
    "            df_train_data = pd.concat(train_data)\n",
    "            df_train_target = pd.concat(train_target)\n",
    "    bnb = BernoulliNB()\n",
    "    bnb.fit(df_train_data, df_train_target)\n",
    "    y_pred = bnb.predict(test_data)\n",
    "    print(\"Test Fold \",x+1, \": Number of mislabeled points out of a total {} points : {}\".format(test_data.shape[0], (test_target != y_pred).sum()))\n",
    "    sum = sum + (test_target != y_pred).sum()\n",
    "    \n",
    "print (sum/n)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original model appears to have some overfitting (25% accuracy when tested with the same dataset, 26.7% accuracy on average when tested using cross-validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Criteria for removing features - remove features one at a time to test impact on model accuracy\n",
    "sums = []\n",
    "columns = []\n",
    "\n",
    "# systematically drop each feature and measure accuracy using a 10 fold split cross-validation\n",
    "for column in data:\n",
    "    data_2 = data\n",
    "    data_3 = data_2.drop([column], axis=1)\n",
    "    n = 10\n",
    "    data_3_split = np.array_split(data_3, n)\n",
    "    target_split = np.array_split(target, n)\n",
    "\n",
    "    #for each fold assign it as the test dataset and fit your model on the remaining data\n",
    "    sum = 0\n",
    "    # for each fold x make it's records test data and concat the rest of the folds into train data\n",
    "    for x in range(n):\n",
    "        train_data = []\n",
    "        train_target = []\n",
    "   \n",
    "        for y in range(n):\n",
    "            if y == x :\n",
    "                test_data = data_3_split[x]\n",
    "                test_target = target_split[y]\n",
    "            else:\n",
    "                train_data.append(data_3_split[y])\n",
    "                train_target.append(target_split[y])\n",
    "                df_train_data = pd.concat(train_data)\n",
    "                df_train_target = pd.concat(train_target)\n",
    "        #fit the model with the train data        \n",
    "        bnb = BernoulliNB()\n",
    "        bnb.fit(df_train_data, df_train_target)\n",
    "        #calculate predictions based on results\n",
    "        y_pred = bnb.predict(test_data)\n",
    "        #print(\"Test Fold \",x+1, \": Number of mislabeled points out of a total {} points : {}\".format(test_data.shape[0], (test_target != y_pred).sum()))\n",
    "        #\n",
    "        sum = sum + (test_target != y_pred).sum()\n",
    "        \n",
    "        \n",
    "    sum_average = sum/n\n",
    "    sums.extend([sum_average])\n",
    "    columns.extend([column])\n",
    "    #print(column,\":\",sum_average)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns_sums_sort = column_sums.sort(['sum_average'], ascending=[0])\n",
    "print(columns_sums_sort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Conclusions: Model seems to perform even less well on data that is not included in training set. It wasn't performing that well anyway but is obviously still overfitted. - Average 30.5% mislabeled (versus 25% when whole dataset trained)\n",
    "\n",
    "for column\n",
    "1. Including all of the features created from the text files resulted in test fold accuracy between 5-75%. This is hugely variable result \n",
    "2. Removing features not represented in the message fields (ie. feature.sum > 0) changed the accuracy in the test folds to between 58-88%. This is still not a very accurate model given that it should be 50% accurate by chance. \n",
    "3. Removing features that show up less than 2 times did not seem to improve the accuracy.\n",
    "4. Adding exclamation points as a feature accuracy 62-83%\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First new classifier: Only including features that hurt the average when removed\n",
    "\n",
    "keywords_df = columns_sums_sort[columns_sums_sort.sum_average > 30.5]\n",
    "print(keywords_df.count())\n",
    "keywords = keywords_df['columns']\n",
    "data_class1 = pd.DataFrame()\n",
    "for key in keywords:\n",
    "    # spaces around the key to get the word,not just pattern matching.\n",
    "    data_class1[str(key)] = sentiment_raw.message_cleaned.str.contains(' ' + str(key) + ' ', case=False).astype(int)\n",
    "\n",
    "    \n",
    "# Model and test on whole dataset\n",
    "bnb = BernoulliNB()\n",
    "# Fit our model to the data.\n",
    "bnb.fit(data_class1, target)\n",
    "# Classify, storing the result in a new variable.\n",
    "y_pred = bnb.predict(data_class1)\n",
    "\n",
    "# Display our results.\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(data_class1.shape[0],(target != y_pred).sum())) \n",
    "    \n",
    "#create data folds and test\n",
    "n= 10\n",
    "data_class1_split = np.array_split(data_class1, n)\n",
    "target_split = np.array_split(target, n)\n",
    "sum=0\n",
    "for x in range(n):\n",
    "    train_data = []\n",
    "    train_target = []\n",
    "    for y in range(n):\n",
    "        if y == x :\n",
    "            test_data = data_class1_split[x]\n",
    "            test_target = target_split[y]\n",
    "        else:\n",
    "            train_data.append(data_class1_split[y])\n",
    "            train_target.append(target_split[y])\n",
    "            df_train_data = pd.concat(train_data)\n",
    "            df_train_target = pd.concat(train_target)\n",
    "    #fit the model with the train data        \n",
    "    bnb = BernoulliNB()\n",
    "    bnb.fit(df_train_data, df_train_target)\n",
    "    #calculate predictions based on results\n",
    "    y_pred = bnb.predict(test_data)\n",
    "    print(\"Test Fold \",x+1, \": Number of mislabeled points out of a total {} points : {}\".format(test_data.shape[0], (test_target != y_pred).sum()))\n",
    "    sum = sum + (test_target != y_pred).sum()\n",
    "            \n",
    "sum_average = sum/n\n",
    "print(\"Average number wrong:\",sum_average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier 1 conclusions:\n",
    "1. Slightly more accurate than original model\n",
    "2. Does not appear to be overfit - the number of mislabeled points when the same dataset is the same as the average of the different fold tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: Adjusted Classifier 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First new classifier: Only including features that hurt the average when removed - less stringent\n",
    "keywords_df = columns_sums_sort[columns_sums_sort.sum_average >= 31]\n",
    "print(keywords_df.count())\n",
    "keywords = keywords_df['columns']\n",
    "data_class2 = pd.DataFrame()\n",
    "for key in keywords:\n",
    "    # spaces around the key to get the word,not just pattern matching.\n",
    "    data_class2[str(key)] = sentiment_raw.message_cleaned.str.contains(' ' + str(key) + ' ', case=False).astype(int)\n",
    "\n",
    "# Model and test on whole dataset\n",
    "bnb = BernoulliNB()\n",
    "# Fit our model to the data.\n",
    "bnb.fit(data_class2, target)\n",
    "# Classify, storing the result in a new variable.\n",
    "y_pred = bnb.predict(data_class2)    \n",
    "# Display results.\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(data_class2.shape[0],(target != y_pred).sum()))    \n",
    "n= 10\n",
    "\n",
    "data_class2_split = np.array_split(data_class2, n)\n",
    "target_split = np.array_split(target, n)\n",
    "sum=0\n",
    "for x in range(n):\n",
    "    train_data = []\n",
    "    train_target = []\n",
    "    for y in range(n):\n",
    "        if y == x :\n",
    "            test_data = data_class2_split[x]\n",
    "            test_target = target_split[y]\n",
    "        else:\n",
    "            train_data.append(data_class2_split[y])\n",
    "            train_target.append(target_split[y])\n",
    "            df_train_data = pd.concat(train_data)\n",
    "            df_train_target = pd.concat(train_target)\n",
    "    #fit the model with the train data        \n",
    "    bnb = BernoulliNB()\n",
    "    bnb.fit(df_train_data, df_train_target)\n",
    "    #calculate predictions based on results\n",
    "    y_pred = bnb.predict(test_data)\n",
    "    print(\"Test Fold \",x+1, \": Number of mislabeled points out of a total {} points : {}\".format(test_data.shape[0], (test_target != y_pred).sum()))\n",
    "    sum = sum + (test_target != y_pred).sum()\n",
    "            \n",
    "sum_average = sum/n\n",
    "print(\"Average number wrong:\",sum_average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier 2 conclusions:\n",
    "1. Not very accurate\n",
    "2. Does not appear to be overfit - the number of mislabeled points when the same dataset is the same as the average of the different fold tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Classifier3 - add punctuation classifiers to classifier 1\n",
    "data_class3 = data_class1\n",
    "data_class3['exclamation'] = sentiment_raw.message.str.contains('!')\n",
    "data_class3['not'] = sentiment_raw.message_cleaned.str.contains('not')\n",
    "\n",
    "# Model and test on whole dataset\n",
    "bnb = BernoulliNB()\n",
    "# Fit our model to the data.\n",
    "bnb.fit(data_class3, target)\n",
    "# Classify, storing the result in a new variable.\n",
    "y_pred = bnb.predict(data_class3)    \n",
    "# Display results.\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(data_class3.shape[0],(target != y_pred).sum()))\n",
    "\n",
    "data_class3_split = np.array_split(data_class1, n)\n",
    "target_split = np.array_split(target, n)\n",
    "\n",
    "    \n",
    "n= 10\n",
    "sum=0\n",
    "for x in range(n):\n",
    "    train_data = []\n",
    "    train_target = []\n",
    "    for y in range(n):\n",
    "        if y == x :\n",
    "            test_data = data_class3_split[x]\n",
    "            test_target = target_split[y]\n",
    "        else:\n",
    "            train_data.append(data_class3_split[y])\n",
    "            train_target.append(target_split[y])\n",
    "            df_train_data = pd.concat(train_data)\n",
    "            df_train_target = pd.concat(train_target)\n",
    "    #fit the model with the train data        \n",
    "    bnb = BernoulliNB()\n",
    "    bnb.fit(df_train_data, df_train_target)\n",
    "    #calculate predictions based on results\n",
    "    y_pred = bnb.predict(test_data)\n",
    "    print(\"Test Fold \",x+1, \": Number of mislabeled points out of a total {} points : {}\".format(test_data.shape[0], (test_target != y_pred).sum()))\n",
    "    sum = sum + (test_target != y_pred).sum()\n",
    "            \n",
    "sum_average = sum/n\n",
    "print(\"Average number wrong:\",sum_average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier 3 conclusions:\n",
    "1. Seems to be the most accurate\n",
    "2. Does not appear to be overfit - the number of mislabeled points when the same dataset is the same as the average of the different fold tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Classifier4 - remove all but a few classifiers\n",
    "data_class4 = pd.DataFrame()\n",
    "\n",
    "data_class4['worst'] = sentiment_raw.message_cleaned.str.contains('worst')\n",
    "data_class4['not'] = sentiment_raw.message_cleaned.str.contains('not')\n",
    "\n",
    "\n",
    "# Model and test on whole dataset\n",
    "bnb = BernoulliNB()\n",
    "# Fit our model to the data.\n",
    "bnb.fit(data_class4, target)\n",
    "# Classify, storing the result in a new variable.\n",
    "y_pred = bnb.predict(data_class4)    \n",
    "# Display results.\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(data_class4.shape[0],(target != y_pred).sum()))\n",
    "\n",
    "n= 10\n",
    "\n",
    "data_class4_split = np.array_split(data_class4, n)\n",
    "target_split = np.array_split(target, n)\n",
    "sum=0\n",
    "for x in range(n):\n",
    "    train_data = []\n",
    "    train_target = []\n",
    "    for y in range(n):\n",
    "        if y == x :\n",
    "            test_data = data_class4_split[x]\n",
    "            test_target = target_split[y]\n",
    "        else:\n",
    "            train_data.append(data_class4_split[y])\n",
    "            train_target.append(target_split[y])\n",
    "            df_train_data = pd.concat(train_data)\n",
    "            df_train_target = pd.concat(train_target)\n",
    "    #fit the model with the train data        \n",
    "    bnb = BernoulliNB()\n",
    "    bnb.fit(df_train_data, df_train_target)\n",
    "    #calculate predictions based on results\n",
    "    y_pred = bnb.predict(test_data)\n",
    "    print(\"Test Fold \",x+1, \": Number of mislabeled points out of a total {} points : {}\".format(test_data.shape[0], (test_target != y_pred).sum()))\n",
    "    sum = sum + (test_target != y_pred).sum()\n",
    "            \n",
    "sum_average = sum/n\n",
    "print(\"Average number wrong:\",sum_average)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier 4 conclusions:\n",
    "1. Not very accurate\n",
    "2. Does not appear to be overfit - the number of mislabeled points when the same dataset is the same as the average of the different fold tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#classifier 5: try removing features who's absence seemed to improve the model in analysis above\n",
    "data_class5 = data\n",
    "#data_class5.drop(['waste','lacked','sick','poor','work','mediocre','liked','slow','gold','like','right','recommend',\\\n",
    "#                  'authentic','convenient','pretty','better','worst'],axis=1,inplace=True)\n",
    "data_class5['exclamation'] = sentiment_raw.message.str.contains('!')\n",
    "data_class5['not'] = sentiment_raw.message_cleaned.str.contains('not')\n",
    "\n",
    "# Model and test on whole dataset\n",
    "bnb = BernoulliNB()\n",
    "# Fit our model to the data.\n",
    "bnb.fit(data_class5, target)\n",
    "# Classify, storing the result in a new variable.\n",
    "y_pred = bnb.predict(data_class5)    \n",
    "# Display results.\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(data_class5.shape[0],(target != y_pred).sum()))\n",
    "\n",
    "\n",
    "n= 10\n",
    "\n",
    "data_class5_split = np.array_split(data_class5, n)\n",
    "target_split = np.array_split(target, n)\n",
    "sum=0\n",
    "for x in range(n):\n",
    "    train_data = []\n",
    "    train_target = []\n",
    "    for y in range(n):\n",
    "        if y == x :\n",
    "            test_data = data_class5_split[x]\n",
    "            test_target = target_split[y]\n",
    "        else:\n",
    "            train_data.append(data_class5_split[y])\n",
    "            train_target.append(target_split[y])\n",
    "            df_train_data = pd.concat(train_data)\n",
    "            df_train_target = pd.concat(train_target)\n",
    "    #fit the model with the train data        \n",
    "    bnb = BernoulliNB()\n",
    "    bnb.fit(df_train_data, df_train_target)\n",
    "    #calculate predictions based on results\n",
    "    y_pred = bnb.predict(test_data)\n",
    "    print(\"Test Fold \",x+1, \": Number of mislabeled points out of a total {} points : {}\".format(test_data.shape[0], (test_target != y_pred).sum()))\n",
    "    sum = sum + (test_target != y_pred).sum()\n",
    "            \n",
    "sum_average = sum/n\n",
    "print(\"Average number wrong:\",sum_average)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier 5 conclusions:\n",
    "1. Appears to be overfit - the number of mislabeled points when the same dataset is used to both fit and test the model is much better than the average when folds are used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Conclusions:\n",
    "1. Classifier 3 seems to be the best model with the highest accuracy (73.2%) on average when cross-validated. \n",
    "2. Based on the differences in average accuracy for cross-validated tests and the accuracy of the model when testing against the training dataset, both the original model and the 5th adjusted model appear to be overfit\n",
    "2. The features that have the greatest impact are almost all words from the positive text dataset, along with the presence of exclamation points and the word \"not.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
